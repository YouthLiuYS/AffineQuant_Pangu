{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "06ee93aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lys/miniconda3/envs/affinequant/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/lys/miniconda3/envs/affinequant/lib/python3.10/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/home/lys/miniconda3/envs/affinequant/lib/python3.10/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/home/lys/miniconda3/envs/affinequant/lib/python3.10/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/home/lys/miniconda3/envs/affinequant/lib/python3.10/site-packages/auto_gptq/nn_modules/triton_utils/kernels.py:411: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  def forward(ctx, input, qweight, scales, qzeros, g_idx, bits, maxq):\n",
      "/home/lys/miniconda3/envs/affinequant/lib/python3.10/site-packages/auto_gptq/nn_modules/triton_utils/kernels.py:419: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "  def backward(ctx, grad_output):\n",
      "/home/lys/miniconda3/envs/affinequant/lib/python3.10/site-packages/auto_gptq/nn_modules/triton_utils/kernels.py:461: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  @custom_fwd(cast_inputs=torch.float16)\n",
      "CUDA extension not installed.\n",
      "CUDA extension not installed.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import random\n",
    "import numpy as np\n",
    "from models.LMClass import LMClass\n",
    "import torch\n",
    "import time\n",
    "import math\n",
    "from datautils import get_loaders\n",
    "from lm_eval import evaluator\n",
    "from pprint import pprint\n",
    "from parallel_utils import map_layers_to_multi_gpus, get_lowest_occupied_gpu\n",
    "import torch.nn as nn\n",
    "from quantize.affinequant import affinequant\n",
    "from tqdm import tqdm\n",
    "import utils\n",
    "from pathlib import Path\n",
    "from categories import subcategories, categories\n",
    "\n",
    "from models.int_llama_layer import QuantLlamaDecoderLayer\n",
    "from models.int_opt_layer import QuantOPTDecoderLayer\n",
    "from quantize.int_linear import QuantLinear\n",
    "# try:\n",
    "#     from llava.model import *   # required for llava\n",
    "# except ImportError:\n",
    "#     print(\"If want to quantize llave models, you should manually install llava from https://github.com/haotian-liu/LLaVA\")\n",
    "\n",
    "import pdb\n",
    "import subprocess\n",
    "\n",
    "def get_gpu_memory_usage():\n",
    "    command = \"nvidia-smi --query-gpu=memory.used --format=csv,nounits,noheader\"\n",
    "    output = subprocess.check_output(command, shell=True).decode().strip()\n",
    "    memory_usage = [int(x) for x in output.split('\\n')]\n",
    "    return memory_usage\n",
    "\n",
    "DEV = torch.device('cuda:0')\n",
    "\n",
    "def benchmark(model, input_ids, check=False):\n",
    "    input_ids = input_ids.to(model.gpus[0] if hasattr(model, 'gpus') else DEV)\n",
    "    torch.cuda.synchronize()\n",
    "\n",
    "    cache = {'past': None}\n",
    "    def clear_past(i):\n",
    "        def tmp(layer, inp, out):\n",
    "            if cache['past']:\n",
    "                cache['past'][i] = None\n",
    "        return tmp\n",
    "    if \"llama\" not in str(type(model)).lower():\n",
    "        for i, layer in enumerate(model.model.decoder.layers):\n",
    "            layer.register_forward_hook(clear_past(i))\n",
    "    else:\n",
    "        for i, layer in enumerate(model.model.layers):\n",
    "            layer.register_forward_hook(clear_past(i))\n",
    "\n",
    "    print('Benchmarking ...')\n",
    "\n",
    "    if check:\n",
    "        loss = nn.CrossEntropyLoss()\n",
    "        tot = 0.\n",
    "\n",
    "    def sync():\n",
    "        if hasattr(model, 'gpus'):\n",
    "            for gpu in model.gpus:\n",
    "                torch.cuda.synchronize(gpu)\n",
    "        else:\n",
    "            torch.cuda.synchronize()\n",
    "    max_memory = 0\n",
    "    with torch.no_grad():\n",
    "        attention_mask = torch.ones((1, input_ids.numel()), device=DEV)\n",
    "        times = []\n",
    "        for i in range(input_ids.numel()):\n",
    "            tick = time.time()\n",
    "            out = model(\n",
    "                input_ids[:, i].reshape(-1,1),\n",
    "                past_key_values=cache['past'],\n",
    "                attention_mask=attention_mask[:, :(i + 1)].reshape((1, -1))\n",
    "            )\n",
    "            sync()\n",
    "            times.append(time.time() - tick)\n",
    "            print(i, times[-1])\n",
    "            if i == 999:\n",
    "                gpu_memory_usage = get_gpu_memory_usage()\n",
    "            max_memory = max(max_memory,torch.cuda.max_memory_allocated() / 1024 /1024)\n",
    "            if check and i != input_ids.numel() - 1:\n",
    "                tot += loss(out.logits[0].to(DEV), input_ids[:, (i + 1)].to(DEV)).float()\n",
    "            cache['past'] = list(out.past_key_values)\n",
    "            del out\n",
    "        sync()\n",
    "        import numpy as np\n",
    "        print('Median:', np.median(times))\n",
    "        if check:\n",
    "            print('PPL:', torch.exp(tot / (input_ids.numel() - 1)).item())\n",
    "            print('max memory(MiB):',max_memory)\n",
    "            print(\"Running Memory(MiB):\", gpu_memory_usage[0])\n",
    "\n",
    "\n",
    "def opt_multigpu(model, gpus):\n",
    "    model.model.decoder.embed_tokens = model.model.decoder.embed_tokens.to(gpus[0])\n",
    "    model.model.decoder.embed_positions = model.model.decoder.embed_positions.to(gpus[0])\n",
    "    if hasattr(model.model.decoder, 'project_in') and model.model.decoder.project_in:\n",
    "        model.model.decoder.project_in = model.model.decoder.project_in.to(gpus[0])\n",
    "    if hasattr(model.model.decoder, 'project_out') and model.model.decoder.project_out:\n",
    "        model.model.decoder.project_out = model.model.decoder.project_out.to(gpus[-1])\n",
    "    if hasattr(model.model.decoder, 'final_layer_norm') and model.model.decoder.final_layer_norm:\n",
    "        model.model.decoder.final_layer_norm = model.model.decoder.final_layer_norm.to(gpus[-1])\n",
    "    import copy\n",
    "    model.lm_head = copy.deepcopy(model.lm_head).to(gpus[-1])\n",
    "\n",
    "    cache = {'mask': None}\n",
    "\n",
    "    class MoveModule(nn.Module):\n",
    "        def __init__(self, module):\n",
    "            super().__init__()\n",
    "            self.module = module\n",
    "            self.dev = next(iter(self.module.parameters())).device\n",
    "        def forward(self, *inp, **kwargs):\n",
    "            inp = list(inp)\n",
    "            if inp[0].device != self.dev:\n",
    "                inp[0] = inp[0].to(self.dev)\n",
    "            if cache['mask'] is None or cache['mask'].device != self.dev:\n",
    "                cache['mask'] = kwargs['attention_mask'].to(self.dev)\n",
    "            kwargs['attention_mask'] = cache['mask']\n",
    "            tmp = self.module(*inp, **kwargs)\n",
    "            return tmp\n",
    "\n",
    "    layers = model.model.decoder.layers\n",
    "    pergpu = math.ceil(len(layers) / len(gpus))\n",
    "    for i in range(len(layers)):\n",
    "        layers[i] = MoveModule(layers[i].to(gpus[i // pergpu]))\n",
    "\n",
    "    model.gpus = gpus\n",
    "\n",
    "\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "net_choices = [\n",
    "    \"opt-125m\",\n",
    "    \"opt-1.3b\",\n",
    "    \"opt-2.7b\",\n",
    "    \"opt-6.7b\",\n",
    "    \"opt-13b\",\n",
    "    \"opt-30b\",\n",
    "    \"opt-66b\",\n",
    "    \"llama-7b\",\n",
    "    \"llama-13b\",\n",
    "    \"llama-30b\",\n",
    "    \"llama-65b\",\n",
    "    \"Llama-2-7b\",\n",
    "    \"Llama-2-13b\",\n",
    "    \"Llama-2-70b\",\n",
    "    \"Llama-2-7b-chat\",\n",
    "    \"Llama-2-13b-chat\",\n",
    "    \"llava-llama-2-13b-chat-lightning-preview\",\n",
    "    \"falcon-180b\",\n",
    "    \"falcon-7b\",\n",
    "]\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(lm, args, logger):\n",
    "    results = {}\n",
    "    if args.multigpu:\n",
    "        if \"opt\" in args.net.lower():\n",
    "            map_layers_to_multi_gpus(lm.model.model.decoder.layers)\n",
    "            input_device = lm.model.model.decoder.layers[0].device\n",
    "            output_device = lm.model.model.decoder.layers[-1].device\n",
    "            lm._device = input_device\n",
    "            assert input_device == output_device\n",
    "            lm.model.model.decoder.embed_positions.to(input_device)\n",
    "            lm.model.model.decoder.embed_tokens.to(input_device)\n",
    "            lm.model.model.decoder.final_layer_norm.to(output_device)\n",
    "            lm.model.lm_head.to(output_device)\n",
    "\n",
    "        elif \"llama\" in args.net.lower():\n",
    "            map_layers_to_multi_gpus(lm.model.model.layers)\n",
    "            input_device = lm.model.model.layers[0].device\n",
    "            output_device = lm.model.model.layers[-1].device\n",
    "            assert input_device == output_device\n",
    "            lm._device = input_device\n",
    "            lm.model.model.embed_tokens.to(input_device)\n",
    "            lm.model.model.norm.to(output_device)\n",
    "            lm.model.lm_head.to(output_device)\n",
    "        elif \"falcon\" in args.net.lower():\n",
    "            map_layers_to_multi_gpus(lm.model.transformer.h)\n",
    "            input_device = lm.model.transformer.h[0].device\n",
    "            output_device = lm.model.transformer.h[-1].device\n",
    "            assert input_device == output_device\n",
    "            lm._device = input_device\n",
    "            lm.model.transformer.word_embeddings.to(input_device)\n",
    "            lm.model.transformer.ln_f.to(output_device)\n",
    "            lm.model.lm_head.to(output_device)\n",
    "    else:\n",
    "        if \"opt\" in args.net.lower():\n",
    "            lm.model.model.decoder = lm.model.model.decoder.to(lm.device)\n",
    "        elif \"llama\" in args.net.lower():\n",
    "            lm.model = lm.model.to(lm.device)\n",
    "        elif \"falcon\" in args.net.lower():\n",
    "            lm.model.transformer = lm.model.transformer.to(lm.device)\n",
    "\n",
    "\n",
    "    if args.eval_ppl:\n",
    "        for dataset in [\"wikitext2\", \"ptb\", \"c4\",\"ptb-new\",'c4-new']:\n",
    "            cache_testloader = f'{args.cache_dir}/testloader_{args.model_family}_{dataset}_all.cache'\n",
    "            if os.path.exists(cache_testloader):\n",
    "                testloader = torch.load(cache_testloader)\n",
    "                logger.info(f\"load calibration from {cache_testloader}\")\n",
    "            else:\n",
    "                dataloader, testloader = get_loaders(\n",
    "                    dataset,\n",
    "                    seed=args.seed,\n",
    "                    model=args.model,\n",
    "                    seqlen=lm.seqlen,\n",
    "                )\n",
    "                torch.save(testloader, cache_testloader)\n",
    "            if \"c4\" in dataset:\n",
    "                testenc = testloader\n",
    "            else:\n",
    "                testenc = testloader.input_ids\n",
    "\n",
    "            nsamples = testenc.numel() // lm.seqlen\n",
    "            use_cache = lm.model.config.use_cache\n",
    "            lm.model.config.use_cache = False\n",
    "            lm.model.eval()\n",
    "            nlls = []\n",
    "            for i in tqdm(range(nsamples)):\n",
    "                batch = testenc[:, (i * lm.seqlen) : ((i + 1) * lm.seqlen)].to(lm.device)\n",
    "                if \"opt\" in args.net.lower():\n",
    "                    outputs = lm.model.model.decoder(batch)\n",
    "                elif \"llama\" in args.net.lower():\n",
    "                    outputs = lm.model.model(batch)\n",
    "                elif \"falcon\" in args.model:\n",
    "                    outputs = lm.model.transformer(batch)\n",
    "                hidden_states = outputs[0]\n",
    "                logits = lm.model.lm_head(hidden_states)\n",
    "                shift_logits = logits[:, :-1, :]\n",
    "                shift_labels = testenc[:, (i * lm.seqlen) : ((i + 1) * lm.seqlen)][\n",
    "                    :, 1:\n",
    "                ].to(lm.model.lm_head.weight.device)\n",
    "                loss_fct = nn.CrossEntropyLoss()\n",
    "                loss = loss_fct(\n",
    "                    shift_logits.view(-1, shift_logits.size(-1)),\n",
    "                    shift_labels.view(-1),\n",
    "                )\n",
    "                neg_log_likelihood = loss.float() * lm.seqlen\n",
    "                nlls.append(neg_log_likelihood)\n",
    "                if i == args.limit:\n",
    "                    break\n",
    "\n",
    "            ppl = torch.exp(torch.stack(nlls).sum() / (nsamples * lm.seqlen))\n",
    "            logger.info(f'{dataset} : {ppl.item()}')\n",
    "            lm.model.config.use_cache = use_cache\n",
    "            results[dataset] = ppl.item()\n",
    "    if args.tasks != \"\":\n",
    "        if 'hendrycksTest' in args.tasks:\n",
    "            mmlu_results = evaluator.simple_evaluate(\n",
    "                lm,\n",
    "                tasks='hendrycksTest',\n",
    "                num_fewshot=5,\n",
    "                limit=None if args.limit == -1 else args.limit,\n",
    "            )\n",
    "            tasks_list = [i for i in args.tasks.replace('hendrycksTest', '').split(\",\") if i != \"\"]\n",
    "            if len(tasks_list) !=0:\n",
    "                t_results = evaluator.simple_evaluate(\n",
    "                    lm,\n",
    "                    tasks=args.tasks.replace('hendrycksTest,', ''),\n",
    "                    num_fewshot=args.num_fewshot,\n",
    "                    limit=None if args.limit == -1 else args.limit,\n",
    "                )\n",
    "        else:\n",
    "            t_results = evaluator.simple_evaluate(\n",
    "                lm,\n",
    "                tasks=args.tasks,\n",
    "                num_fewshot=args.num_fewshot,\n",
    "                limit=None if args.limit == -1 else args.limit,\n",
    "            )\n",
    "        results.update(t_results)\n",
    "        logger.info(results)\n",
    "        pprint(results)\n",
    "        # for test of MMLU\n",
    "        if 'hendrycksTest' in args.tasks:\n",
    "            all_cors = []\n",
    "            all_cors_norm = []\n",
    "            subcat_cors = {subcat: [] for subcat_lists in subcategories.values() for subcat in subcat_lists}\n",
    "            cat_cors = {cat: [] for cat in categories}\n",
    "            cat_cors_norm = {cat: [] for cat in categories}\n",
    "            for key in mmlu_results['results'].keys():\n",
    "                if not 'hendrycksTest' in key:\n",
    "                    continue\n",
    "                subject = key.split('-')[-1]\n",
    "                cors = mmlu_results['results'][key]['acc']\n",
    "                cors_norm = mmlu_results['results'][key]['acc_norm']\n",
    "                subcats = subcategories[subject]\n",
    "                for subcat in subcats:\n",
    "                    subcat_cors[subcat].append(cors)\n",
    "                    for key in categories.keys():\n",
    "                        if subcat in categories[key]:\n",
    "                            cat_cors[key].append(cors)\n",
    "                            cat_cors_norm[key].append(cors_norm)\n",
    "                    all_cors.append(cors)\n",
    "                    all_cors_norm.append(cors_norm)\n",
    "                    \n",
    "            for cat in cat_cors:\n",
    "                cat_acc = np.mean(cat_cors[cat])\n",
    "                logger.info(\"Average accuracy {:.4f} - {}\".format(cat_acc, cat))\n",
    "            weighted_acc = np.mean(all_cors)\n",
    "            logger.info(\"Average accuracy: {:.4f}\".format(weighted_acc))               \n",
    "    return results\n",
    "\n",
    "\n",
    "def main():\n",
    "    import argparse\n",
    "\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--model\", type=str, help=\"model name of model path\")\n",
    "    parser.add_argument(\"--cache_dir\", default=\"./cache\", type=str, help=\"cache dir of dataset, leading to faster debug\")\n",
    "    parser.add_argument(\"--output_dir\", default=\"../log/\", type=str, help=\"direction of logging file\")\n",
    "    parser.add_argument(\"--save_dir\", default=None, type=str, help=\"direction for saving fake quantization model\")\n",
    "    parser.add_argument(\"--resume\", type=str, default=None)\n",
    "    parser.add_argument(\"--real_quant\", default=False, action=\"store_true\",)\n",
    "    parser.add_argument(\"--calib_dataset\",type=str,default=\"wikitext2\",\n",
    "        choices=[\"wikitext2\", \"ptb\", \"c4\", \"mix\",\"pile\"],\n",
    "        help=\"Where to extract calibration data from.\",\n",
    "    )\n",
    "    parser.add_argument(\"--nsamples\", type=int, default=128, help=\"Number of calibration data samples.\")\n",
    "    parser.add_argument(\"--batch_size\", type=int, default=1, help=\"batch size.\")\n",
    "    parser.add_argument(\"--seed\", type=int, default=2, help=\"Seed for sampling the calibration data.\")\n",
    "    parser.add_argument(\"--tasks\", default=\"\")\n",
    "    parser.add_argument(\"--eval_ppl\", action=\"store_true\")\n",
    "    parser.add_argument(\"--num_fewshot\", type=int, default=0)\n",
    "    parser.add_argument(\"--wbits\", type=int, default=4)\n",
    "    parser.add_argument(\"--abits\", type=int, default=4)\n",
    "    parser.add_argument(\"--group_size\", type=int, default=None)\n",
    "    parser.add_argument(\"--alpha\", type=float, default=0.5)\n",
    "    parser.add_argument(\"--let_lr\", type=float, default=5e-3)\n",
    "    parser.add_argument(\"--lwc_lr\", type=float, default=1e-2)\n",
    "    parser.add_argument(\"--wd\", type=float, default=0)\n",
    "    parser.add_argument(\"--epochs\", type=int, default=10)\n",
    "    parser.add_argument(\"--let\",default=False, action=\"store_true\",help=\"activate learnable equivalent transformation\")\n",
    "    parser.add_argument(\"--lwc\",default=False, action=\"store_true\",help=\"activate learnable weight clipping\")\n",
    "    parser.add_argument(\"--aug_loss\", default=False, action=\"store_true\", help=\"calculate additional loss with same input\")\n",
    "    parser.add_argument(\"--symmetric\",default=False, action=\"store_true\", help=\"symmetric quantization\")\n",
    "    parser.add_argument(\"--use_matrix\", default=False, action=\"store_true\", help=\"qkt affine mateix or not\")\n",
    "    parser.add_argument(\"--use_ln_matrix\",default=False, action=\"store_true\", help=\"layernorm vector or matrix\")\n",
    "    parser.add_argument('--sf',\"--stability_factor\",type=float, default=1.0, help=\"stability factor for gradual mask\")\n",
    "    parser.add_argument(\"--a_dynamic_method\", type=str, default=\"per_token\", choices=[\"per_token\"])\n",
    "    parser.add_argument(\"--w_dynamic_method\", type=str, default=\"per_channel\", choices=[\"per_channel\"])\n",
    "    parser.add_argument(\"--limit\", type=int, default=-1)\n",
    "    parser.add_argument(\"--multigpu\", action=\"store_true\", help=\"at eval, map model to multiple gpus\")\n",
    "    parser.add_argument(\"--deactive_amp\", action=\"store_true\", help=\"deactivate AMP when 8<=bits<16\")\n",
    "    parser.add_argument(\"--net\", type=str, default=None, choices=net_choices)\n",
    "    parser.add_argument(\"--act-scales\", type=str, default=None)\n",
    "    parser.add_argument(\"--act-shifts\", type=str, default=None)\n",
    "    parser.add_argument(\n",
    "        '--benchmark', type=int, default=0,\n",
    "        help='Number of tokens to use for benchmarking.'\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--check', action='store_true',\n",
    "        help='Whether to compute perplexity during benchmarking for verification.'\n",
    "    )\n",
    "\n",
    "    args = parser.parse_args()\n",
    "    random.seed(args.seed)\n",
    "    np.random.seed(args.seed)\n",
    "    torch.manual_seed(args.seed)\n",
    "    torch.cuda.manual_seed(args.seed)\n",
    "    args.dtype = torch.float32\n",
    "\n",
    "    # check\n",
    "    if args.epochs > 0:\n",
    "        assert args.lwc or args.let\n",
    "        \n",
    "    if (args.wbits<16 and args.wbits>=8) or (args.abits<16 and args.abits>=8):\n",
    "        args.deactive_amp = True\n",
    "\n",
    "    # init logger\n",
    "    if args.output_dir:\n",
    "        Path(args.output_dir).mkdir(parents=True, exist_ok=True)\n",
    "    if args.cache_dir:\n",
    "        Path(args.cache_dir).mkdir(parents=True, exist_ok=True)\n",
    "    if args.save_dir:\n",
    "        Path(args.save_dir).mkdir(parents=True, exist_ok=True)\n",
    "    output_dir = Path(args.output_dir)\n",
    "    logger = utils.create_logger(output_dir)\n",
    "    logger.info(args)\n",
    "    \n",
    "    # load model\n",
    "    if args.net is None:\n",
    "        args.net = args.model.split('/')[-1]\n",
    "    # assert args.net in net_choices\n",
    "    args.model_family = args.net.split('-')[0]\n",
    "    lm = LMClass(args)\n",
    "    lm.seqlen = 2048\n",
    "    lm.model.eval()\n",
    "    print(lm.model.parameters())\n",
    "    for param in lm.model.parameters():\n",
    "        param.requires_grad = False"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "affinequant",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
